{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled1.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyNLGn/NKPCJVpARFm97XMlF"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"vXI1dqG_4cVJ","executionInfo":{"status":"ok","timestamp":1618364084908,"user_tz":-540,"elapsed":1246,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#pytorch아님, 기존 torch api를 파이썬 형으로 바꾼것이 import torch\n","import torch\n","#nn이라는 것을 torch에서 import함\n","from torch import nn\n","#dataloader로 사용하기 위한 것\n","from torch.utils.data import DataLoader\n","#파이토치에서 사용하는 컴퓨터비전 알고리즘들이 구현되어있는게 torchvision\n","from torchvision import datasets\n","#dataset관리\n","#torch를 효과적으로 사용할 수 있는 유틸리티가 많이 있음\n","from torchvision.transforms import ToTensor, Lambda, Compose\n","#matplotlib.pyplot을 plt로 쓰겠다\n","import matplotlib.pyplot as plt"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2G5APy1M6666","executionInfo":{"status":"ok","timestamp":1618362494489,"user_tz":-540,"elapsed":4621,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"5375c0b7-8f91-4dbd-f5ee-c29b3d49a232"},"source":["pip install torch"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch) (1.19.5)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GagzqyPw6_W_","executionInfo":{"status":"ok","timestamp":1618364088318,"user_tz":-540,"elapsed":1139,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#이미 mnist에서 train,test를 분리해놓았으니 train=true로 train 데이터를 가져옴\n","training_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=True,\n","    download=True,\n","    transform=ToTensor(),\n",")"],"execution_count":28,"outputs":[]},{"cell_type":"code","metadata":{"id":"IkiK9pcJ7Osh","executionInfo":{"status":"ok","timestamp":1618364090044,"user_tz":-540,"elapsed":1120,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#train=false이므로 test데이터 가져옴\n","test_data = datasets.FashionMNIST(\n","    root=\"data\",\n","    train=False,\n","    download=True,\n","    transform=ToTensor(),\n",")"],"execution_count":29,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"72wbxU2v7UZ4","executionInfo":{"status":"ok","timestamp":1618364091970,"user_tz":-540,"elapsed":1150,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"88b7af99-ff05-4179-c67b-78fa8868a932"},"source":["batch_size = 64\n","\n","# Create data loaders.\n","#앞에서 불러온 train data를 dataloader클래스로 만들음\n","#train 데이터는 그냥 데이터 그자체(row data)\n","#dataloader는 데이터에 스케쥴링(학습 데이터 사이즈, 배치사이즈)등을 설정한 데이터, 학습을 위한 환경 설정\n","train_dataloader = DataLoader(training_data, batch_size=batch_size)\n","test_dataloader = DataLoader(test_data, batch_size=batch_size)\n","#몇개인지, 채널 길이, 채널 높이, 가로\n","#mnist는 28*28이미지, 1은 채널(그레이스케일, rgb면 3), 64는 batch size\n","#는 데이터들에 대한 label, 어떤 이미지인지 나타내는 데이터\n","#보통 X는 데이터, y는 레이블로 이루어져있음\n","#y는 그냥 1차원 벡터 데이터\n","#dtype는 데이터 타입, int64임\n","for X, y in test_dataloader:\n","    print(\"Shape of X [N, C, H, W]: \", X.shape)\n","    print(\"Shape of y: \", y.shape, y.dtype)\n","    break"],"execution_count":30,"outputs":[{"output_type":"stream","text":["Shape of X [N, C, H, W]:  torch.Size([64, 1, 28, 28])\n","Shape of y:  torch.Size([64]) torch.int64\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t9geTWtP9m-j","executionInfo":{"status":"ok","timestamp":1618364095282,"user_tz":-540,"elapsed":1821,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"60fca631-f28d-4bd4-f905-8e46c36d9bb5"},"source":["# Get cpu or gpu device for training.\n","#device는 cuda로 해라 cuda가 사용 가능하면, 아니면 cpu를 써라\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","print(\"Using {} device\".format(device))\n","\n","# Define model\n","#class만들때 nn.module에서 상속받아 모델을 만듬\n","#파이토치에서는 하나의 레이어는 모듈, 그 레이어를 여러개 모은것도 모듈, 따라서 nn의 기본 단위는 module\n","#Module의 초기화 값을 우리가 상속받아 재정의함\n","# Define model\n","class NeuralNetwork(nn.Module):\n","    def __init__(self):\n","        #슈퍼틀래스 nn.moulde 먼저 초기화하고 뒤에있는 코드들 실행함, 부모클래스의 속성들을 모두 먼저 정의함\n","        super(NeuralNetwork, self).__init__()\n","        self.flatten = nn.Flatten()\n","        #리니얼 렐루 스택이라는 인스턴스를 시퀀셜 클래스를 이용해 만듬\n","        #시퀀셜은 순서대로라는 의미\n","        #리니어 렐루 리니어 렐루 등을 순서대로 실행하는 함수\n","        #괄호안의 레이어들을 순서대로 실행함\n","        self.linear_relu_stack = nn.Sequential(\n","            #첫번째 실행\n","            nn.Linear(28*28, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 512),\n","            nn.ReLU(),\n","            nn.Linear(512, 10),\n","            #마지막 실행\n","            nn.ReLU()\n","        )\n","\n","    def forward(self, x):\n","        #포워드라는 함수가 nn.module에 이미 있는데 우리가 재정의함\n","        #먼저 플래튼\n","        x = self.flatten(x)\n","        #리니어 렐루 스택 레이어 통과해 계산\n","        #위에서 정의한 함수\n","        #실제 함수 실행 과정\n","        logits = self.linear_relu_stack(x)\n","        return logits\n","\n","#Neuralnetwork 클래스를 device 메모리로 올림\n","model = NeuralNetwork().to(device)\n","print(model)"],"execution_count":31,"outputs":[{"output_type":"stream","text":["Using cpu device\n","NeuralNetwork(\n","  (flatten): Flatten(start_dim=1, end_dim=-1)\n","  (linear_relu_stack): Sequential(\n","    (0): Linear(in_features=784, out_features=512, bias=True)\n","    (1): ReLU()\n","    (2): Linear(in_features=512, out_features=512, bias=True)\n","    (3): ReLU()\n","    (4): Linear(in_features=512, out_features=10, bias=True)\n","    (5): ReLU()\n","  )\n",")\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"GEiWzSEKBbE5","executionInfo":{"status":"ok","timestamp":1618364320835,"user_tz":-540,"elapsed":788,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#이제 w각 layer의 w벡터 학습시키기\n","#딥러닝의 정확도 판단하는 방법이 loss\n","loss_fn = nn.CrossEntropyLoss()\n","#기울기 보고 로스가 작아지는 방식으로 테트\n","optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)"],"execution_count":32,"outputs":[]},{"cell_type":"code","metadata":{"id":"4coe4KZf_ipj","executionInfo":{"status":"ok","timestamp":1618365596892,"user_tz":-540,"elapsed":631,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["#학습방법 정의\n","#산에서 하산하기 위한 방법은 정의했지만 아직 실행은 하지 않음\n","#여기 트레인 함수에서 실행함\n","def train(dataloader, model, loss_fn, optimizer):\n","    size = len(dataloader.dataset)\n","    #데이터와 레이블 뿐만 아니라 뱃치도 가져옴, 한번에 데이터 몇개씩 가져올것인지\n","    for batch, (X,y) in enumerate(dataloader):\n","      #X,y를 디바이스로 보냄\n","      X, y = X.to(device), y.to(device)\n","\n","      # Compute prediction error\n","      #이건 앞에서 정의했던 model\n","      #뉴럴넷 클래스를 인스턴스해 cpu/gpu메모리로 보낸것\n","      #모델 클래스에 X를 받아 포워드 함수 실행한 것\n","      #modle(X)이면 포워드 함수를 실행하라는 것이 엔엔.모듈을 실행함\n","      #nn.module에 그 기능이 구현되어 있음\n","      #pred에는 0~9까지 label에 대한 확률이 리턴됨\n","      #그 10개의 확률이 64개 저장되어 있음\n","      pred = model(X)\n","      #예측한 확률이 정답과 얼마나 정확한지 loss에 계산\n","      #프레딕션이 y랑 일치하면 로스는 0, 하지만 처음에는 같지 않음\n","      #로스클수록 성능 안좋음\n","      loss = loss_fn(pred, y)\n","\n","      # Backpropagation\n","\n","      #옵티마이저에 그라디언트를 0으로 초기화해라\n","      #산에서 내려오고 있을 때 현재 지점까지 오는데 필요했던 방향이나 크기는 잊어라\n","      #optimizer에는 sgb를 씀\n","      #여기까지는 어떻게 내려갈지 머릿속에서 계산하는 과정\n","      optimizer.zero_grad()\n","      #실제로 w를 학습시킴\n","      #실제로 산을 내려감\n","      loss.backward()\n","      #현재 내가 loss.backward에서 내려왔으니까 최정상에서 어떻게 내려왔는지 저장\n","      #급하게 내려와서 천천히 내려가야 하는 정보 저장\n","      #지금까지 어떻게 내려왔는지 히스토리 저장 - 천천히 내려가거나 빠르게 내려가거나 \n","      optimizer.step()\n","\n","      #batch가 0,100,200 등등 일 때\n","      if batch % 100 == 0:\n","          #우리가 얼마만큼 학습되고 있는지 보여줌\n","          loss, current = loss.item(), batch * len(X)\n","          print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"id":"rZmsoXv7CdJr","executionInfo":{"status":"ok","timestamp":1618365600815,"user_tz":-540,"elapsed":621,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}}},"source":["def test(dataloader, model):\n","    size = len(dataloader.dataset)\n","    model.eval()\n","    test_loss, correct = 0, 0\n","    #토치 노 그라드를 만족하는 거에 대해서 포문 돌림\n","    #그라디언트를 사용하지 않겠다(학습하지 않겠다)\n","    with torch.no_grad():\n","        for X, y in dataloader:\n","            X, y = X.to(device), y.to(device)\n","            #위에 train네 있던 학습과정 없이 로스만 구함                                           \n","            pred = model(X)\n","            test_loss += loss_fn(pred, y).item()\n","            #prediction한거 10개의 확률중에서 첫번째로 큰 확률을 가지는 index를 리턴하는게 argmax\n","            #트루폴스를 보여줌\n","            #64개중에 30개가 맞으면 30개를 float로 바꾸면 트루는 1이므로 다 더하면 30, .item()으로 30만 값에 더함 \n","            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n","    test_loss /= size\n","    #이게 확률\n","    #사이즈는 데이터셋 전체 데이터 개수\n","    #if size가 640이면 for문은 64배치이므로 10번돌음\n","    #코렉트에는 10번의 포문동안 돌고, 그걸 640으로 나누면 확률나옴\n","    #다맞추면 1됨\n","    correct /= size\n","    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"],"execution_count":38,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RFFWRQFMG5gh","executionInfo":{"status":"ok","timestamp":1618365677275,"user_tz":-540,"elapsed":57444,"user":{"displayName":"오규석","photoUrl":"","userId":"11076778257915679187"}},"outputId":"e30d8d76-17b1-49c5-e196-d2120885209f"},"source":["epochs = 5\n","for t in range(epochs):\n","    print(f\"Epoch {t+1}\\n-------------------------------\")\n","    train(train_dataloader, model, loss_fn, optimizer)\n","    test(test_dataloader, model)\n","print(\"Done!\")"],"execution_count":39,"outputs":[{"output_type":"stream","text":["Epoch 1\n","-------------------------------\n","loss: 2.301810  [    0/60000]\n","loss: 2.295594  [ 6400/60000]\n","loss: 2.283173  [12800/60000]\n","loss: 2.276067  [19200/60000]\n","loss: 2.251948  [25600/60000]\n","loss: 2.254463  [32000/60000]\n","loss: 2.241603  [38400/60000]\n","loss: 2.236965  [44800/60000]\n","loss: 2.232320  [51200/60000]\n","loss: 2.181247  [57600/60000]\n","Test Error: \n"," Accuracy: 45.4%, Avg loss: 0.034668 \n","\n","Epoch 2\n","-------------------------------\n","loss: 2.223769  [    0/60000]\n","loss: 2.214528  [ 6400/60000]\n","loss: 2.201533  [12800/60000]\n","loss: 2.195928  [19200/60000]\n","loss: 2.113989  [25600/60000]\n","loss: 2.144141  [32000/60000]\n","loss: 2.122798  [38400/60000]\n","loss: 2.120017  [44800/60000]\n","loss: 2.122030  [51200/60000]\n","loss: 2.005776  [57600/60000]\n","Test Error: \n"," Accuracy: 45.4%, Avg loss: 0.032582 \n","\n","Epoch 3\n","-------------------------------\n","loss: 2.115533  [    0/60000]\n","loss: 2.093925  [ 6400/60000]\n","loss: 2.077704  [12800/60000]\n","loss: 2.070770  [19200/60000]\n","loss: 1.898275  [25600/60000]\n","loss: 1.984956  [32000/60000]\n","loss: 1.945772  [38400/60000]\n","loss: 1.959799  [44800/60000]\n","loss: 1.975775  [51200/60000]\n","loss: 1.772932  [57600/60000]\n","Test Error: \n"," Accuracy: 45.4%, Avg loss: 0.029915 \n","\n","Epoch 4\n","-------------------------------\n","loss: 1.980774  [    0/60000]\n","loss: 1.952619  [ 6400/60000]\n","loss: 1.942580  [12800/60000]\n","loss: 1.925002  [19200/60000]\n","loss: 1.670470  [25600/60000]\n","loss: 1.839376  [32000/60000]\n","loss: 1.763458  [38400/60000]\n","loss: 1.815678  [44800/60000]\n","loss: 1.838352  [51200/60000]\n","loss: 1.576905  [57600/60000]\n","Test Error: \n"," Accuracy: 45.6%, Avg loss: 0.027590 \n","\n","Epoch 5\n","-------------------------------\n","loss: 1.860431  [    0/60000]\n","loss: 1.835889  [ 6400/60000]\n","loss: 1.832281  [12800/60000]\n","loss: 1.807735  [19200/60000]\n","loss: 1.497167  [25600/60000]\n","loss: 1.733045  [32000/60000]\n","loss: 1.625141  [38400/60000]\n","loss: 1.707350  [44800/60000]\n","loss: 1.730944  [51200/60000]\n","loss: 1.449695  [57600/60000]\n","Test Error: \n"," Accuracy: 46.3%, Avg loss: 0.025859 \n","\n","Done!\n"],"name":"stdout"}]}]}