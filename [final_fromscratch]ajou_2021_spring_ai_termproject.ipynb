{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"[final_fromscratch]ajou_2021_spring_ai_termproject.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"F1bPPSg1NZJm"},"source":["# 재구축 데이터셋 Scratch\n","\n"]},{"cell_type":"code","metadata":{"id":"dHIByFLlYcpy"},"source":["import torch\n","import torch.utils.data as data\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torchvision import transforms, datasets, models\n","import torch.optim as optim\n","import torchvision.transforms as transforms\n","from torch.utils.data import TensorDataset\n","from PIL import Image\n","import numpy as np\n","from tqdm import tqdm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nfU_Y_gSMc42","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a9b82959-4f69-4ab4-db90-7370c6fae7e2"},"source":["# 구축된 .npy파일을 Pytorch DataLoader을 사용할 수 있도록 CUSTOM DATASET을 만듬.\n","import numpy as np\n","from google.colab import drive\n","\n","drive.mount('/content/drive')\n","class CUB200(data.Dataset):\n","    def __init__(self, Train, transform = None):\n","        super(CUB200, self).__init__()\n","        \"\"\"\n","        Train : bool = True\n","        \"\"\"\n","        if Train == True:\n","            load_data = np.load('/content/drive/My Drive/colab_data/cub/final/train_data.npy')\n","            load_label = np.load('/content/drive/My Drive/colab_data/cub/final/train_label.npy')\n","            self.image = load_data\n","            self.label = load_label\n","            self.transform = transform\n","            \n","        else:\n","            load_data = np.load('/content/drive/My Drive/colab_data/cub/final/test_data.npy')\n","            #train data를 똑같이 쓰는거\n","            #or 트레인 데이터 반으로 나눠서 쓰기\n","            load_label = np.load('/content/drive/My Drive/colab_data/cub/final/test_label.npy')\n","            self.image = load_data\n","            self.label = load_label\n","            self.transform = transform\n","\n","    def __getitem__(self, index):\n","        img, target = self.image[index], self.label[index]\n","        img = Image.fromarray(img)\n","\n","        if self.transform is not None:\n","            img = self.transform(img)\n","\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.image)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Mounted at /content/drive\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"245P8QOlMjkD","outputId":"71eceee7-7d36-48ad-9d57-eae7e9c9374b"},"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["cuda\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"5TkTeovfMkvI"},"source":["# train_data에만 data augmentaion을 적용\n","transform_train = transforms.Compose([\n","        #이거 주석 풀면 데이터 어규먼테이션 가능\n","        # transforms.RandomCrop(224),\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])\n","\n","transform_test = transforms.Compose([\n","        transforms.ToTensor(),\n","        transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ytid3c0vMlsI"},"source":["# CUSTOM DATASET을 이용하여 train_loader, test_loader을 구축\n","\n","batch_size = 8\n","\n","train_loader = torch.utils.data.DataLoader(\n","    dataset = CUB200(True, transform = transform_train),\n","    batch_size = batch_size,\n","    shuffle = True\n",")\n","\n","test_loader = torch.utils.data.DataLoader(\n","    dataset = CUB200(False, transform = transform_test),\n","    batch_size = batch_size,\n","    shuffle = False\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0Vlnqo9sM1DH"},"source":["def training_model(model, criterion, optimizer, scheduler, num_epochs = 25):\n","\n","\n","    for epoch in range(num_epochs):\n","        scheduler.step()\n","\n","        running_loss = 0.0\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs, labels = inputs.to(device), labels.to(device)\n","\n","            optimizer.zero_grad()\n","\n","            outputs = model(inputs)\n","            loss = criterion(outputs, labels)\n","            loss.backward()\n","            optimizer.step()\n","\n","            running_loss += loss.item()\n","\n","            if i % 20 == 19:\n","                print('[%d, %5d] loss: %.7f' %\n","                    (epoch + 1, (i + 1), running_loss / 20))\n","                running_loss = 0.0\n","        \n","        train_correct = 0\n","        train_total = 0\n","        for i, data in enumerate(train_loader, 0):\n","            inputs, labels = data\n","            inputs = inputs.squeeze()\n","            inputs, labels = inputs.to(device), labels.to(device)\n","            outputs = model(inputs)\n","\n","            _, predicted = torch.max(outputs.data, 1)\n","            train_total += labels.size(0)\n","            train_correct += (predicted == labels).sum().item()\n","\n","        print('[%d epoch] Accuracy of the network on the train images: %d %%' %\n","              (epoch + 1, 100 * train_correct / train_total))\n","        \n","    print(\"End Training do it eval_accuracy\")\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FmDHs5-65Ycs"},"source":["def eval_accuracy(model):\n","    #프레딕션 할떄 트레인 모델로 만든 딥러닝 모델이 프레딕션한 레이블을 리스트로 만들어서 암호화해 올리면 됨\n","    class_correct = list(0. for i in range(50))\n","    class_total = list(0. for i in range(50))\n","\n","    correct = 0\n","    total = 0\n","    \n","    model.eval()\n","    with torch.no_grad():\n","        for i, data in enumerate(test_loader, 0):\n","            images, labels = data\n","            images, labels = images.to(device), labels.to(device)\n","            outputs = model(images)\n","            _, predicted = torch.max(outputs, 1)\n","            c = (predicted == labels).squeeze()\n","                    \n","            for i in range(labels.shape[0]):\n","                label = labels[i]\n","                class_correct[label] += c[i].item()\n","                class_total[label] += 1\n","                \n","                total += labels.size(0)\n","                correct += (predicted == labels).sum().item()\n","\n","    print('Accuracy of the network on test images: %d %%' % (\n","        100 * correct / total))            \n","                \n","    return "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJh_SvoiODRd"},"source":["class Scratch_Net(nn.Module):\n","    def __init__(self):\n","        super().__init__()\n","        self.pool = nn.MaxPool2d(2, 2)\n","        self.conv1 = nn.Conv2d(in_channels=3, out_channels=128, kernel_size=5, stride=1)\n","        self.conv2 = nn.Conv2d(in_channels=128, out_channels=256, kernel_size=5, stride=1)\n","        self.conv3 = nn.Conv2d(in_channels=256, out_channels=256, kernel_size=5, stride=1)\n","        self.conv4 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1)\n","        # self.conv5 = nn.Conv2d(in_channels=256, out_channels=128, kernel_size=3, stride=1)\n","        self.fc1 = nn.Linear(128 * 13 * 13, 512)\n","        self.fc2 = nn.Linear(512, 50)\n","        # self.fc3 = nn.Linear(256, 50)\n","\n","    def forward(self, x):\n","        # print(\"연산 전\", x.size()) #[8, 3, 224, 224]\n","        x = self.pool(F.relu(self.conv1(x)))\n","        # print(\"conv1 연산 후\", x.size()) #[8, 128, 110, 110]\n","        x = self.pool(F.relu(self.conv2(x)))\n","        # print(\"conv2 연산 후\",x.size()) # [8, 256, 55, 55]\n","        x = self.pool(F.relu(self.conv3(x)))\n","        # print(\"conv3 연산 후\",x.size()) # [8, 256, 26, 26]\n","        x = self.pool(F.relu(self.conv4(x)))\n","        # print(\"conv4 연산 후\",x.size()) # [8, 256, 26, 26]\n","        # x = self.pool(F.relu(self.conv5(x)))\n","        # print(\"conv5 연산 후\",x.size()) # [8, 256, 26, 26]\n","        x = x.view(-1, 128 * 13 * 13)\n","        # print(\"차원 감소 후\", x.size()) # [8, 18432, 1, 1]\n","        x = F.relu(self.fc1(x))\n","        # print(\"fc1 연산 후\", x.size()) # [8, 512, 1, 1]\n","        # x = F.relu(self.fc2(x))\n","        # print(\"fc2 연산 후\", x.size())\n","        x = self.fc2(x)\n","        # print(\"fc3 연산 후\", x.size()) # [8, 50, 1, 1]\n","        return x\n","\n","\n","model_ft = Scratch_Net()\n","output = model_ft(torch.randn(8, 3, 256, 256))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u5cIKBkGNF1a"},"source":["num_epochs = 25\n","model_ft.to(device)\n","criterion = nn.CrossEntropyLoss()\n","# optimizer = optim.Adam(model_ft.parameters(), lr = 0.001)\n","optimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n","lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size = 7, gamma = 0.1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bjcQV0A6M3bx","outputId":"748148af-2336-42cf-ece8-17c0c1029f00"},"source":["model_ft = training_model(model_ft, criterion, optimizer, lr_scheduler, num_epochs)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py:134: UserWarning: Detected call of `lr_scheduler.step()` before `optimizer.step()`. In PyTorch 1.1.0 and later, you should call them in the opposite order: `optimizer.step()` before `lr_scheduler.step()`.  Failure to do this will result in PyTorch skipping the first value of the learning rate schedule. See more details at https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\n","  \"https://pytorch.org/docs/stable/optim.html#how-to-adjust-learning-rate\", UserWarning)\n"],"name":"stderr"},{"output_type":"stream","text":["[1,    20] loss: 3.9171464\n","[1,    40] loss: 3.9141876\n","[1,    60] loss: 3.9140873\n","[1,    80] loss: 3.9142504\n","[1,   100] loss: 3.9101089\n","[1 epoch] Accuracy of the network on the train images: 3 %\n","[2,    20] loss: 3.8982859\n","[2,    40] loss: 3.8910889\n","[2,    60] loss: 3.8954141\n","[2,    80] loss: 3.9026462\n","[2,   100] loss: 3.9048022\n","[2 epoch] Accuracy of the network on the train images: 7 %\n","[3,    20] loss: 3.8426473\n","[3,    40] loss: 3.8235675\n","[3,    60] loss: 3.8123295\n","[3,    80] loss: 3.8093471\n","[3,   100] loss: 3.8514423\n","[3 epoch] Accuracy of the network on the train images: 8 %\n","[4,    20] loss: 3.6842944\n","[4,    40] loss: 3.6595941\n","[4,    60] loss: 3.7709548\n","[4,    80] loss: 3.6936137\n","[4,   100] loss: 3.7190937\n","[4 epoch] Accuracy of the network on the train images: 9 %\n","[5,    20] loss: 3.5292681\n","[5,    40] loss: 3.4710718\n","[5,    60] loss: 3.4932063\n","[5,    80] loss: 3.5737743\n","[5,   100] loss: 3.4992071\n","[5 epoch] Accuracy of the network on the train images: 15 %\n","[6,    20] loss: 3.2156302\n","[6,    40] loss: 3.1842796\n","[6,    60] loss: 3.3736075\n","[6,    80] loss: 3.3131475\n","[6,   100] loss: 3.1900584\n","[6 epoch] Accuracy of the network on the train images: 21 %\n","[7,    20] loss: 2.8837160\n","[7,    40] loss: 2.9465758\n","[7,    60] loss: 2.6495723\n","[7,    80] loss: 2.5071782\n","[7,   100] loss: 2.4527214\n","[7 epoch] Accuracy of the network on the train images: 43 %\n","[8,    20] loss: 2.2100470\n","[8,    40] loss: 2.2349719\n","[8,    60] loss: 2.2549873\n","[8,    80] loss: 2.3688302\n","[8,   100] loss: 2.0902281\n","[8 epoch] Accuracy of the network on the train images: 48 %\n","[9,    20] loss: 1.9435422\n","[9,    40] loss: 1.8612443\n","[9,    60] loss: 1.9302283\n","[9,    80] loss: 2.0961940\n","[9,   100] loss: 2.0027749\n","[9 epoch] Accuracy of the network on the train images: 55 %\n","[10,    20] loss: 1.5878127\n","[10,    40] loss: 1.5641472\n","[10,    60] loss: 1.7347218\n","[10,    80] loss: 1.7355260\n","[10,   100] loss: 1.8922828\n","[10 epoch] Accuracy of the network on the train images: 65 %\n","[11,    20] loss: 1.3820373\n","[11,    40] loss: 1.5447499\n","[11,    60] loss: 1.2609439\n","[11,    80] loss: 1.5415653\n","[11,   100] loss: 1.5247556\n","[11 epoch] Accuracy of the network on the train images: 70 %\n","[12,    20] loss: 1.1458409\n","[12,    40] loss: 1.1992215\n","[12,    60] loss: 1.4508625\n","[12,    80] loss: 1.4045862\n","[12,   100] loss: 1.1810469\n","[12 epoch] Accuracy of the network on the train images: 73 %\n","[13,    20] loss: 1.0247219\n","[13,    40] loss: 0.9781677\n","[13,    60] loss: 1.1169965\n","[13,    80] loss: 1.1451407\n","[13,   100] loss: 1.0137563\n","[13 epoch] Accuracy of the network on the train images: 76 %\n","[14,    20] loss: 0.8220160\n","[14,    40] loss: 0.7132484\n","[14,    60] loss: 0.7651821\n","[14,    80] loss: 0.8262917\n","[14,   100] loss: 0.5539588\n","[14 epoch] Accuracy of the network on the train images: 88 %\n","[15,    20] loss: 0.5389333\n","[15,    40] loss: 0.6417944\n","[15,    60] loss: 0.4983553\n","[15,    80] loss: 0.5581587\n","[15,   100] loss: 0.5936329\n","[15 epoch] Accuracy of the network on the train images: 90 %\n","[16,    20] loss: 0.5127198\n","[16,    40] loss: 0.5558707\n","[16,    60] loss: 0.5419888\n","[16,    80] loss: 0.5358263\n","[16,   100] loss: 0.4606719\n","[16 epoch] Accuracy of the network on the train images: 92 %\n","[17,    20] loss: 0.4425967\n","[17,    40] loss: 0.5600589\n","[17,    60] loss: 0.4655356\n","[17,    80] loss: 0.5313603\n","[17,   100] loss: 0.5212195\n","[17 epoch] Accuracy of the network on the train images: 92 %\n","[18,    20] loss: 0.4361260\n","[18,    40] loss: 0.5094845\n","[18,    60] loss: 0.5064981\n","[18,    80] loss: 0.4982166\n","[18,   100] loss: 0.4042659\n","[18 epoch] Accuracy of the network on the train images: 93 %\n","[19,    20] loss: 0.3969912\n","[19,    40] loss: 0.4436266\n","[19,    60] loss: 0.4356662\n","[19,    80] loss: 0.5293759\n","[19,   100] loss: 0.3848827\n","[19 epoch] Accuracy of the network on the train images: 93 %\n","[20,    20] loss: 0.3457656\n","[20,    40] loss: 0.4279249\n","[20,    60] loss: 0.4076410\n","[20,    80] loss: 0.4021922\n","[20,   100] loss: 0.4715611\n","[20 epoch] Accuracy of the network on the train images: 94 %\n","[21,    20] loss: 0.4714300\n","[21,    40] loss: 0.4138026\n","[21,    60] loss: 0.4061746\n","[21,    80] loss: 0.2403705\n","[21,   100] loss: 0.3581669\n","[21 epoch] Accuracy of the network on the train images: 94 %\n","[22,    20] loss: 0.3961391\n","[22,    40] loss: 0.3497508\n","[22,    60] loss: 0.3687244\n","[22,    80] loss: 0.4104897\n","[22,   100] loss: 0.3145249\n","[22 epoch] Accuracy of the network on the train images: 93 %\n","[23,    20] loss: 0.4316404\n","[23,    40] loss: 0.4013439\n","[23,    60] loss: 0.4167858\n","[23,    80] loss: 0.2569210\n","[23,   100] loss: 0.3670898\n","[23 epoch] Accuracy of the network on the train images: 94 %\n","[24,    20] loss: 0.4024366\n","[24,    40] loss: 0.3861541\n","[24,    60] loss: 0.3330737\n","[24,    80] loss: 0.4373148\n","[24,   100] loss: 0.3310456\n","[24 epoch] Accuracy of the network on the train images: 94 %\n","[25,    20] loss: 0.3862232\n","[25,    40] loss: 0.3006464\n","[25,    60] loss: 0.4555058\n","[25,    80] loss: 0.3872173\n","[25,   100] loss: 0.3063424\n","[25 epoch] Accuracy of the network on the train images: 94 %\n","End Training do it eval_accuracy\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"xaCtc86RPepo","colab":{"base_uri":"https://localhost:8080/","height":53},"outputId":"c844d626-2624-4db2-c497-bc20e4fafe6d"},"source":["eval_accuracy(model_ft)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Accuracy of the network on the 500 test images: 13 %\n"],"name":"stdout"},{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'End model test'"]},"metadata":{"tags":[]},"execution_count":17}]},{"cell_type":"code","metadata":{"id":"Q-MdTqeI6KOA"},"source":[""],"execution_count":null,"outputs":[]}]}